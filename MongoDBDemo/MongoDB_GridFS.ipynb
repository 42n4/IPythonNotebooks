{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MongoDB GridFS demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "by Dr. Rikard Sandstr√∂m, rsandstroem@kpmg.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have not done so already, I recommend that you first look at \"MongoDB_demo.ipynb\" in this repository. It explains the basics in MongoDB and introduces pymongo.\n",
    "\n",
    "This demonstration shows how to use GridFS for storing large files in MongoDB. MongoDB has a document size limit of a few MB. GridFS circumvents this by splitting the file in smaller chunks. I find it convenient for storing binary data such as images and video. For more information please see https://docs.mongodb.org/manual/core/gridfs/\n",
    "\n",
    "The demonstration stores a scanned pdf in GridFS, then extracts text from the pdf and adds that as another \"column\" of the same collection. This allows retrieval of pdf's whos content match user defined search terms. This use case was brought to life by my frustration over searching through piles of papers when filling out my tax returns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a MongoClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, import things we will need. Use pymongo to connect to the \"test\" database on port 27017 of the local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "import bson\n",
    "import gridfs\n",
    "client = MongoClient('localhost', 27017)\n",
    "db = client.test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a GridFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start from a clean slate, begin by dropping the collections GridFS might have created earlier. GridFS creates a \"files\" collection, containing meta data, and the \"chunks\" collection, containing small chunks of the large files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "db.drop_collection('fs.files')\n",
    "db.drop_collection('fs.chunks')\n",
    "fs = gridfs.GridFS(db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hello World"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's insert an object and see if we can retrieve it. The object in this case is just a string containing \"Hello World\". Add some additional free text description called \"text\" and a tag field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "put returned:\t56fe679cb143762514358152\n",
      "database contains:\tHello World\n"
     ]
    }
   ],
   "source": [
    "a = fs.put(\"Hello World\", text='I am a tiny document', tags=['foo', 'bar'], username='skywalker')\n",
    "print 'put returned:\\t', a\n",
    "print 'database contains:\\t', fs.get(a).read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract text from a scanned PDF file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try something more complex. \"scansmpl.pdf\" is a free sample of a scanned pdf file containing text in image format. For pdf files already containing text in text format we can just grab the text with pdftotext. If this does not work we need to run and OCR to optically reconstruct characters from the image, which is much slower. \n",
    "\n",
    "Todo: use better OCR software, preferably without dropping to system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text consists of 1230 words\n"
     ]
    }
   ],
   "source": [
    "filename = 'scansmpl.pdf'\n",
    "tags = ['default']\n",
    "os.system('pdftotext ' + filename + ' temp.txt')\n",
    "text = open('temp.txt').read()\n",
    "if len(text)<2: # No text found, so we need to do an OCR \n",
    "    os.system('ocrmypdf ' + filename + ' ocr.pdf')\n",
    "    os.system('pdftotext ocr.pdf temp.txt')\n",
    "    text = open('temp.txt').read()\n",
    "print 'Text consists of', len(text), 'words'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert the document to the data base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First convert the pdf to binary format. Then insert the binary version of the original file into mongo, together with the text extraction we did in the previous step and the file name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectId('56fe67a3b143762514358154')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = bson.Binary(open(filename).read())\n",
    "fs.put(filename=filename, data=data, text=text, tags=tags, username='dvader')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect data base contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use command list() to get an array of files that were inserted. There should only be one file at this point, but if you inserted it multiple times it would not repeat in the array, so let's count replicas just to be sure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'scansmpl.pdf']\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print fs.list()\n",
    "print fs.find({\"filename\" : \"scansmpl.pdf\"}).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that the meta data was stored in fs.files? You can see what each document contains using this collection, like shown below. The \"_id\" links to the inserted object in fs.chunks. Very convenient!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{u'_id': ObjectId('56fe679cb143762514358152'),\n",
       "  u'chunkSize': 261120,\n",
       "  u'length': 11,\n",
       "  u'md5': u'b10a8db164e0754105b7a99be72e3fe5',\n",
       "  u'tags': [u'foo', u'bar'],\n",
       "  u'text': u'I am a tiny document',\n",
       "  u'uploadDate': datetime.datetime(2016, 4, 1, 12, 20, 44, 60000),\n",
       "  u'username': u'skywalker'},\n",
       " {u'_id': ObjectId('56fe67a3b143762514358154'),\n",
       "  u'chunkSize': 261120,\n",
       "  u'filename': u'scansmpl.pdf',\n",
       "  u'length': 21530,\n",
       "  u'md5': u'8d2fc6d280b1385302910fd5162eaad2',\n",
       "  u'tags': [u'default'],\n",
       "  u'text': u'THE SLEREXE COMPANY LIMITED\\nSAPORS LANE\\nTELEPHONE\\n\\nOur Ref.\\n\\n350/PJC/EAC\\n\\nDr.\\n\\nCundall,\\n\\nP.N.\\n\\n.\\n\\nBOOLE\\n\\nBoots\\n\\n-\\n\\nDORSET\\n\\n(94513) 51617\\n\\n-\\n\\n.\\n\\nBHZS SER\\n\\n123456\\n\\nnuax\\n\\n18th\\n\\nJanuary, 1972.\\n\\nMining Surveys Ltd.,\\nHolroyd Road,\\n\\nReading,\\nBerks.\\n\\nDear\\n\\nPete,\\n\\nPermit me\\ntransmission.\\n\\nintroduce you\\n\\nto\\n\\nto\\n\\nfacility\\n\\nthe\\n\\nof\\n\\nfacsimile\\n\\nphotocell is caused to perform a raster scan over\\nvariations of print density on the document\\nsubject copy.\\ncause the photocell to generate an analogous electrical video signal.\\nThis signal is used to modulate a carrier, which is transmitted to a\\nIn\\n\\nfacsimile\\n\\na\\n\\nThe\\n\\nthe\\n\\nremote\\n\\nAt\\n\\ndestination\\nthe\\n\\nremote\\n\\nover\\n\\na\\n\\nradio\\n\\nterminal,\\n\\nor\\n\\ncommunications link.\\n\\ncable\\n\\ndemodulation\\n\\nreconstructs\\n\\nthe video\\n\\ndensity of print produced by a\\nThis device is scanning in a raster scan synchronised\\nAs a result, a facsimile\\nwith that at the transmitting terminal.\\nis\\nof\\nthe\\ndocument\\ncopy\\nsubject\\nproduced.\\n\\nsignal, which is\\nprinting device.\\n\\nProbably\\n\\nused to modulate\\n\\nyou have\\n\\nuses\\n\\nthe\\n\\nfor this\\n\\nfacility\\nYours\\n\\nin your\\n\\norganisation.\\n\\nsincerely,\\n\\nW.\\nP.J.\\n\\nCROSS\\n\\nGroup Leader\\n\\nNo.\\n\\nRexlnered\\nRegistered Office:\\n\\nin\\n\\n-\\n\\nFacsimile Research\\n\\nEngland:\\n\\nNo. 2038\\n\\n80 Vicar-I\\n\\nLune, Illord. Enos.\\n\\n\\x0c',\n",
       "  u'uploadDate': datetime.datetime(2016, 4, 1, 12, 20, 51, 911000),\n",
       "  u'username': u'dvader'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(db.fs.files.find())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search database for a file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you want to get your hands on all documents with a specific filename. Just do a standard find() on the MongoDB, the do whatever you want with the documents (here I print the upload date)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 documents\n",
      "2016-04-01 12:20:51.911000\n"
     ]
    }
   ],
   "source": [
    "cursor = fs.find({\"filename\" : \"scansmpl.pdf\"}).limit(3)\n",
    "print \"Found\", cursor.count(), \"documents\"\n",
    "for doc in cursor:\n",
    "    print doc.uploadDate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to grab the file using the \"_id\" you must convert it to an ObjectId first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectId('56fe67a3b143762514358154')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bson.objectid import ObjectId\n",
    "fs.find_one({'_id': ObjectId('56fe67a3b143762514358154')})._id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are only interested in the last version you could retrieve it like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-04-01 12:20:51.911000\n"
     ]
    }
   ],
   "source": [
    "print fs.get_last_version(\"scansmpl.pdf\").uploadDate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of extracting the text from the scanned pdf earlier was to enable searching for matching substrings to retrieve documents. To do this we need to create an index of the text field. \n",
    "\n",
    "I have yet to figure out how to do this with pymongo, but in the mongo shell this is just one short line (see comment below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "#db.fs.files.createIndex({ \"text\": \"text\" } ) # need to implement this in pymongo, works in mongo shell\n",
    "founddocs = fs.find({\"$text\": { \"$search\": \"Cundall\" }})\n",
    "print founddocs.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the found documents to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i,doc in enumerate(founddocs):\n",
    "    with open(doc.filename+'_'+str(i), 'w') as f:\n",
    "      f.write(fs.get(doc._id).read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now have the original file(s) matching the search terms with a counter suffix in the local folder."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
